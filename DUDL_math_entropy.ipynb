{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqgfUkdXxNwWqyv9yRXZlR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LLewis/ai-deep-learning/blob/main/DUDL_math_entropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entropy formula:\n",
        "          H(p)= - âˆ‘ p(x)log(p(x))\n",
        "                  x\n",
        "\n"
      ],
      "metadata": {
        "id": "v-q9x2LSXpu2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EMI9J6cXWWR",
        "outputId": "757dc27f-1892-4c24-a205-71b1629dee24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong entropy: 0.34657359027997264\n",
            "I think the formula is missing the summation\n",
            "Correct entropy: 0.5623351446188083\n",
            "Another Correct entropy: 0.5623351446188083\n",
            "CORRECT ENTROPY: 1.3862943611198906\n",
            "ANOTHER CORRECT ENTROPY: 1.3862943611198906\n",
            "Manually simplified: 1.3862943611198906\n"
          ]
        }
      ],
      "source": [
        "#import libraries\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#probability of an event happening\n",
        "p = .25   #this event has a 25% chance of happening\n",
        "q = 1 - p         #the probaility of the event not happening is .75 or 1 - p\n",
        "                  #So, I sum all values of x not just the one value of x .25. x from formula above\n",
        "\n",
        "#NOT the correct formula\n",
        "H = -(p*np.log(p))\n",
        "print(('Wrong entropy: ' + str(H)))\n",
        "print('I think the formula is missing the summation')\n",
        "#The isssue is, the code is computing only one entopy for one event - the probability of the event actually happening .25\n",
        "#The code must also create another entopy for one event - not happening\n",
        "\n",
        "#The correct  way to compute entropy\n",
        "#.25 probability of event happening and .75 probability of event not happening\n",
        "x = [ .25, .75]\n",
        "\n",
        "H = 0\n",
        "for p in x:\n",
        "  H += -p*np.log(p)   #sum all values   or can also be written as:  H -= p*np.log(p)\n",
        "print('Correct entropy: ' + str(H))\n",
        "\n",
        "#another correct way, written out for n=2 events  -\n",
        "#Explicitly written out\n",
        "\n",
        "#BINARY CROSS - ENTROPY  - IMPORTANT\n",
        "H = -(p*np.log(p) + (1-p)*np.log(1-p))\n",
        "print('Another Correct entropy: ' + str(H) ) \n",
        "\n",
        "#1.00\n",
        "# .25   event happening-occuring\n",
        "#______\n",
        "#  .75  event not happening - occuring\n",
        "#--------------------------------------------\n",
        "\n",
        "#Cross-entropy set of events\n",
        "\n",
        "#note all probs must sum to 1\n",
        "p = [1, 0] # sum=1\n",
        "q = [.25, .75] #sum=1\n",
        "\n",
        "H=0\n",
        "for i in range(len(p)):\n",
        "  H -= p[i]*np.log(q[i])   #computes the cross-entropy between p and q ---  p times log(q) \n",
        "print ('CORRECT ENTROPY: ' + str(H))  \n",
        "\n",
        "#another correct, written out for n=2 events\n",
        "H = -(p[0]*np.log(q[0])+ p[1]*np.log(q[1]))\n",
        "print('ANOTHER CORRECT ENTROPY: ' + str(H))\n",
        "\n",
        "#simplified\n",
        "H = -np.log(q[0])\n",
        "print('Manually simplified: ' + str(H))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now using pytorch\n",
        "import torch\n",
        "import torch.nn.functional as F   # typical coding in pytorch\n",
        "\n",
        "#note inputs must be Tensors\n",
        "#converting from list to a tensor \n",
        "q_tensor = torch.Tensor(q)\n",
        "p_tensor = torch.Tensor(p)\n",
        "\n",
        "F.binary_cross_entropy(q_tensor, p_tensor)  #binary - computing cross-entropy between two probabilities, binary because there are two options present or absent\n",
        "                                             #sensative to the order of input \"p\" is the category labels (is_cat, isNot_cat)\n",
        "                                             #q is the model predicatability is it a cat or is it not a cat\n",
        "                                             \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4D17vLxovGDJ",
        "outputId": "3e3eb8fa-719d-4e20-fd6c-7d3234b57331"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.3863)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}